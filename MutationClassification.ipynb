{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d94d2bb",
   "metadata": {},
   "source": [
    "SAAPpred script that predicts protein pathogenicty from SAAPdap data, using SciKit-Learn random forests. \n",
    "Goal is to predict SNP or PD with MCC > 0.7.\n",
    "        \n",
    "Best CV models are directly used for final testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports the required libraries and packages \"\"\"\n",
    "\n",
    "import pandas as pd                                                              # Data manipulation in dataframes\n",
    "import numpy as np                                                               # Array manipulation\n",
    "import pickle\n",
    "import hyperopt\n",
    "\n",
    "import random as rd                                                              # Random seed generation\n",
    "import time                                                                      # Time program run time\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch                                             # CV visualise\n",
    "from matplotlib import colors \n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,                                                           # MCC for evaluation\n",
    "    confusion_matrix,                                                            # Confusion matrix for classification evalutation\n",
    "    classification_report                                                        # Return the F1, precision, and recall of a prediction\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import(\n",
    "    GroupKFold                                                                   # K-fold CV with as groups\n",
    "        )\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier                              # SK learn API for classificastion random forests\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK                                       # Functions for minimising cost functions\n",
    "from hyperopt.pyll.base import scope\n",
    "from functools import partial\n",
    "\n",
    "np.set_printoptions(precision = 3,threshold=np.inf, suppress=True)               # Full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "### Split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c3c19c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_data(file_train, file_test):\n",
    "    \"\"\"      \n",
    "     Returns:    Training_Set     Scaled 80% training set split\n",
    "                 Testing_Set      Scaled 20% testing set split\n",
    "            \n",
    "    Opens the scaled training and testing data\n",
    "    \"\"\"\n",
    "    Training_Set = pd.read_csv(file_train, index_col = 0)\n",
    "    Testing_Set = pd.read_csv(file_test,index_col = 0)\n",
    "    \n",
    "    with open(\"seed.txt\", \"r\") as f:\n",
    "        seed = int(f.read().strip())\n",
    "    \n",
    "    return Training_Set, Testing_Set, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_data(Training_Set, Testing_Set):\n",
    "    \"\"\"      \n",
    "    Input:      Training_Set     Scaled 80% training set split\n",
    "                Testing_Set      20% testing set split\n",
    "\n",
    "    Returns:    TrainData        Training features \n",
    "                TrainLabels      Training labels\n",
    "                TestData         Testing features \n",
    "                TestLabels       Testing labels\n",
    "            \n",
    "    Separates training and testing data into features and labels\n",
    "    \"\"\"\n",
    "    TrainData     = Training_Set.drop(['AC Code','dataset'], axis =1)  \n",
    "    TrainLabels   = Training_Set['dataset']\n",
    "    \n",
    "    TestData     = Testing_Set.drop(['AC Code','dataset'], axis =1)  \n",
    "    TestLabels   = Testing_Set['dataset']        \n",
    "    \n",
    "    return (TrainData, TrainLabels, TestData, TestLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a2df",
   "metadata": {},
   "source": [
    "## Group K-fold CV (outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "975ff775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CV(Training_Set, seed):\n",
    "    \"\"\"      \n",
    "    Input:      Training_Set     80% training set split\n",
    "            \n",
    "    Returns:    IT_list         List of training features for each fold\n",
    "                LT_list         List of training class labels for each fold\n",
    "                IV_list         List of validation features for each fold\n",
    "                LV_list         List of validation class labels for each fold\n",
    "\n",
    "    Group K-fold CV with protein groups separated between training and validation sets for each fold. Creates 5 folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    features     = Training_Set.drop(['dataset'], axis =1)         #Features for training\n",
    "    labels       = Training_Set['dataset']                         #Class labels for training\n",
    "    groups       = Training_Set['AC Code'].to_list()               #List of proteins for grouping\n",
    "    \n",
    "    CV           = GroupKFold(n_splits = 5)                        #Creates 5 splits\n",
    "    \n",
    "    IT_list      = []\n",
    "    LT_list      = []\n",
    "    IV_list      = []\n",
    "    LV_list      = []\n",
    "    \n",
    "    for train_idx, val_idx in CV.split(features, labels, groups):       #Generates the indices to be used for a training and validation split. Indicies are unique to train/ val sets\n",
    "\n",
    "        Input_train                        = features.loc[train_idx]    #New dataframe from selected indices\n",
    "        Classes_train                      = labels.loc[train_idx]\n",
    "        Input_train.drop(['AC Code'], axis = 1, inplace = True)         #Group identifer not needed for training\n",
    "\n",
    "                \n",
    "        Input_val                          = features.loc[val_idx]\n",
    "        Classes_val                        = labels.loc[val_idx]\n",
    "        Input_val.drop(['AC Code'], axis   = 1, inplace = True)\n",
    "        \n",
    "        Input_train.reset_index(drop = True, inplace = True)            #Reset index of each set for compatability with balancing\n",
    "        Classes_train.reset_index(drop = True, inplace = True)\n",
    "        Input_val.reset_index(drop = True, inplace = True)\n",
    "        Classes_val.reset_index(drop = True, inplace = True)\n",
    "\n",
    "        IT_list.append(Input_train.sample(frac=1, random_state=seed))          #Shuffles lists, random state to ensure features and labels match for each fold\n",
    "        LT_list.append(Classes_train.sample(frac=1, random_state=seed))\n",
    "        IV_list.append(Input_val.sample(frac=1, random_state=(seed-1)))\n",
    "        LV_list.append(Classes_val.sample(frac=1, random_state=(seed-1)))\n",
    "        \n",
    "\n",
    "    return(IT_list, LT_list, IV_list, LV_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "## Balancing (inner loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "    \"\"\" \n",
    "    Input:        classData  Array of class labels\n",
    "\n",
    "    Returns:      minClass   The label for the minority class\n",
    "                  minSize    The number of items in the minority class\n",
    "                  maxSize    The number of items in the majority class\n",
    "\n",
    "    Find information about class size imbalance\n",
    "    \"\"\"\n",
    "    \n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize  = Minority_count\n",
    "    maxSize  = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize  = Majority_count\n",
    "        maxSize  = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \"\"\" \n",
    "    Input:        inData          Dataframe of input data\n",
    "                  classData       Series of classes assigned\n",
    "                  minorityClass   class label for the minority class\n",
    "                  minoritySize    size of the minority class\n",
    "\n",
    "    Returns:      usedLines       array of indexes that are of interest for a balanced dataset\n",
    "\n",
    "    Perform the actual balancing for a fold between SNPs and PDs\n",
    "    \"\"\"\n",
    "    usedLines = [False] * len(inData)      #Array of false for length of data\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:       #Balance directly with dataframe\n",
    "            usedLines[i] = True            #True lines are SNP\n",
    "            \n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedLines[i] = True\n",
    "            usedCount += 1                 #Set PD lines \"True\", until equal to number of SNP lines\n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(inData, classData, usedLines):\n",
    "    \"\"\"     \n",
    "    Input:      inData         Datframe of input training data\n",
    "                classData      Series of classes assigned to training data\n",
    "                usedLines      Array of line indexes to print\n",
    "\n",
    "    Returns:    input_balance  Dataframe of balanced training features\n",
    "                label_balance  Dataframe of balanced training labels\n",
    "                       \n",
    "    Create dataframe of the input training data and classes used. Index_list tracks the indicies between usedLines and inData, used to pull the required5 lines.\n",
    "    \"\"\"\n",
    "    input_balance = []\n",
    "    label_balance = []\n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(len(usedLines)):\n",
    "        if usedLines[i] == True:\n",
    "            index_list.append(i)\n",
    "             \n",
    "    input_balance = inData.iloc[index_list].reset_index(inplace = False, drop = True)\n",
    "    label_balance = classData.iloc[index_list].reset_index(inplace = False, drop = True) \n",
    "    \n",
    "    return input_balance, label_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27419a33",
   "metadata": {},
   "source": [
    "### Balance for n folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(maxSize, minSize): \n",
    "    \"\"\" \n",
    "    Input:      maxSize     The number of items in the majority class\n",
    "                minSize     The number of items in the minority class\n",
    "\n",
    "    Returns:    BF          Number of balancing folds\n",
    "\n",
    "    Calculate the number of balancing folds needed using ratio of majority to minority class size. Double to ensure sufficient\n",
    "    majority class instances are sampled, then + 1 to make odd to allow weighted vote.\n",
    "    \"\"\"\n",
    "    Divide = maxSize/minSize\n",
    "    BF = ((2 * round(Divide)) + 1)    # ratio to nearest odd integer\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds(BF, inData, classData, minClass, minSize):\n",
    "    \"\"\" \n",
    "    Input:      BF                Number of balancing folds\n",
    "                inData            Datframe of input training data\n",
    "                classData         Series of classes assigned to training data\n",
    "                minClass          The label for the minority class\n",
    "                minSize           The number of items in the minority class\n",
    "                                  \n",
    "    Returns:    Input_folds       List of balanced training feature folds\n",
    "                Output_folds      List of balanced training label folds\n",
    "\n",
    "    Runs balance_data() for the number of balancing folds. Return lists of balanced folds features and labels\n",
    "    where each item is the output of balance_data()\n",
    "    \"\"\"\n",
    "    Input_folds  = []\n",
    "    Output_folds = []\n",
    "\n",
    "    for i in range(BF):\n",
    "        usedLines                    = balance(inData, classData, minClass, minSize)\n",
    "        input_balance, label_balance = balance_data(inData, classData, usedLines)\n",
    "        \n",
    "        Input_folds.append(input_balance)\n",
    "        Output_folds.append(label_balance)\n",
    "            \n",
    "    return Input_folds, Output_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cd1aaa",
   "metadata": {},
   "source": [
    "### RFC hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0840d9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperopt_space():\n",
    "    \"\"\" \n",
    "    Returns:   Space         Parameter space for hyperparameter searching\n",
    "\n",
    "    Define paramater space for Hyperopt to search throug\n",
    "   \"\"\"  \n",
    "    space = {\n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 100)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 3, 9, 1)),\n",
    "        'eta': hp.uniform('eta', 0.45, 0.75),\n",
    "        }\n",
    "     \n",
    "    return space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b7a62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params, Input_folds, Output_folds): \n",
    "    \"\"\" \n",
    "    Input:      params            Search paramaters parsed in fmin function()                     \n",
    "                Input_folds       List of balanced training feature folds\n",
    "                Output_folds      List of balanced training label folds\n",
    "                \n",
    "    Returns:    loss, status      The MCC from evaluating hyperparameters during search\n",
    "\n",
    "    Define the model that Hyperopt will optimise hyperparameters for\n",
    "    \"\"\"     \n",
    "    max_depth = params['max_depth']\n",
    "    eta = params['eta']\n",
    "    num_boost_round = num_boost_round['num_boost_round']\n",
    "    \n",
    "    settings = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'disable_default_eval_metric': 1,\n",
    "        'verbosity': 1,\n",
    "        'max_depth': max_depth,\n",
    "        'eta': eta,\n",
    "            } \n",
    "                    \n",
    "    model = RFC.fit(settings)\n",
    "    #Generates and fits a GBC for each training balanced fold\n",
    "    \n",
    "    pred = model.predict(Output_folds)\n",
    "    MCC = matthews_corrcoef(Output_folds.get_label(), pred.round())\n",
    "                                                                         \n",
    "    return {'loss': -MCC, 'status': STATUS_OK}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796af0e8",
   "metadata": {},
   "source": [
    "### Train RFC on the trainings folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_fitting(BF, Input_folds, Output_folds): \n",
    "    \"\"\" \n",
    "    Input:      BF                Number of balancing folds                      \n",
    "                Input_folds       List of balanced training feature folds\n",
    "                Output_folds      List of balanced training label folds\n",
    "\n",
    "    Returns:    BF_RFC            List of RFCs trained on each balancing fold\n",
    "\n",
    "    Create RFC model that returns probability predictions for each fold, using output of Balance_Folds() as training data\n",
    "    \"\"\"    \n",
    "    BF_RFC = []\n",
    "    for i in range(BF):\n",
    "        BF_RFC.append(RandomForestClassifier(verbose = 0)) #Generates a RFC for each fold's training data\n",
    "        BF_RFC[i].fit(Input_folds[i], Output_folds[i])     #Fits the RFC to each folds' training data\n",
    "        \n",
    "    return BF_RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd278c",
   "metadata": {},
   "source": [
    "#### Validate each RFC on validation set, for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acc41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_validate(BF_RFC, ValData):\n",
    "    \"\"\" \n",
    "    Input:      BF_RFC          List of RFCs trained on balancing folds\n",
    "                ValData         Unseen validation features from CV fold\n",
    "                \n",
    "    Returns:    Prob_matrix     List of arrays. Each item is 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "    \n",
    "    Test the trained RFCs on the test set, then for every instance, outputs the predicted probability for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    Prob_matrix = []\n",
    "    \n",
    "    for i in range(len(BF_RFC)):\n",
    "        Prob = BF_RFC[i].predict_proba(ValData.values) #Predicts the probability of an instance belonging to major or minor class\n",
    "        Prob_matrix.append(Prob)   \n",
    "        \n",
    "    return Prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Vote(Prob_matrix):\n",
    "    \"\"\" \n",
    "    Input:      Prob_matrix     List of arrays. 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "\n",
    "    Returns:    Final_vote      Weighted vote classification\n",
    "\n",
    "    Calculate the final predictions with weighted vote using confidence scores. \n",
    "    Sc = (S0 -T)/(1-T) if S0> T\n",
    "    Sc = (T-S0)/T if S0 < T\n",
    "    \"\"\"\n",
    "    Sc_SNP = []\n",
    "    Sc_PD = []\n",
    "    \n",
    "    for i in range(len(Prob_matrix)):\n",
    "        Sc_SNP.append(Prob_matrix[i][:,0])\n",
    "        Sc_PD.append(Prob_matrix[i][:,1])\n",
    "    \n",
    "    # T = 0.5                                     #Lower threshold gives more sensitivity to PDs over SNPs\n",
    "    # Sc_SNP = []\n",
    "    # Sc_PD = []\n",
    "\n",
    "    # for fold in range(len(Prob_matrix)):        #Calculates SNP Sc all instances in each fold\n",
    "\n",
    "    #     Sc_SNP_fold = []                        #List of the Sc for each fold\n",
    "    #     for value in range(len(Prob_matrix[fold][:,0])):\n",
    "    #         S0 = Prob_matrix[fold][:,0][value]  #Each SNP's confidence in prob matrix fold\n",
    "    #         if S0 < T:\n",
    "    #             Sc = (T - S0)/T\n",
    "    #         elif S0 >= T:\n",
    "    #             Sc = (S0 - T)/(1 - T)        \n",
    "    #         Sc_SNP_fold.append(Sc)              #List of Sc for each fold\n",
    "    #     Sc_SNP.append(Sc_SNP_fold)              #List of folds with Sc\n",
    "\n",
    "    # for fold in range(len(Prob_matrix)):        #Calculates PD Sc all instances in each fold\n",
    "    #     Sc_PD_fold = []\n",
    "    #     for value in range(len(Prob_matrix[fold][:,1])):\n",
    "    #         S0 = Prob_matrix[fold][:,1][value]  #Each PD's confidence in prob matrix fold\n",
    "    #         if S0 < T:\n",
    "    #             Sc = (T - S0)/T\n",
    "    #         elif S0 >= T:\n",
    "    #             Sc = (S0 - T)/(1 - T)        \n",
    "    #         Sc_PD_fold.append(Sc)\n",
    "    #     Sc_PD.append(Sc_PD_fold)\n",
    "        \n",
    "    columnSNP = np.stack(Sc_SNP)                #Covert list of lists to array, shape (5,~539)\n",
    "    columnPD  = np.stack(Sc_PD)\n",
    "\n",
    "    Sum_SNP   = np.sum(columnSNP, axis = 0)     #Sum of all SNP confidence scores. 1D Array\n",
    "    Sum_PD    = np.sum(columnPD, axis = 0)      #Sum of all PD confidence scores. 1D Array\n",
    "    \n",
    "    \n",
    "    Vote_arr  = [] \n",
    "\n",
    "    for i in range(len(Sum_PD)):\n",
    "        if Sum_PD[i] >= Sum_SNP[i]:\n",
    "            Vote_arr.append([1])                #Append PD classifications to list\n",
    "        elif Sum_SNP[i] > Sum_PD[i]:\n",
    "            Vote_arr.append([0])                #Append SNP classifications to list\n",
    "\n",
    "    Final_vote = np.stack(Vote_arr)             #Converts list of arrays to a 2D array\n",
    "    Final_vote = Final_vote.ravel()             #Flattens 2D array to 1D array\n",
    "\n",
    "    return(Final_vote, Sum_PD, Sum_SNP)         #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f36545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalutation(Vallabel, Final_vote):\n",
    "    \"\"\" \n",
    "    Input:      Vallabel           Unseen validation class labels from CV fold\n",
    "                Final_vote         Weighted vote classification\n",
    "\n",
    "    Evaluate each fold with confusion matrix and MCC\n",
    "    \"\"\"\n",
    "    Output_pred = Final_vote\n",
    "    print(f\"-----------------------------------------------------\\n              ***Fold {folds + 1} Evaluation***\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Vallabel, Output_pred)}\")\n",
    "    print(f\"{classification_report(Vallabel, Output_pred)}\\nMCC                {matthews_corrcoef(Vallabel, Output_pred)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e232",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a5a9897",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2606\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2630\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 10",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m minClass, minSize, maxSize  \u001b[39m=\u001b[39m find_minority_class(classData)                            \u001b[39m#Determines imbalance\u001b[39;00m\n\u001b[0;32m     17\u001b[0m BF                          \u001b[39m=\u001b[39m Balance_ratio(maxSize, minSize)                           \u001b[39m#Determins number of balancing folds needed\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m Input_folds, Output_folds   \u001b[39m=\u001b[39m Balance_Folds(BF, inData, classData, minClass, minSize)   \u001b[39m# balance() and balance_data() functions are called under this\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# space                   = hyperopt_space()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m# trials                  = Trials()\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39m# fmin_objective          = partial(objective,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[39m# final_param.append(trials.argmin)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m BF_RFC                      \u001b[39m=\u001b[39m BF_fitting(BF, Input_folds, Output_folds)\n",
      "Cell \u001b[1;32mIn[9], line 19\u001b[0m, in \u001b[0;36mBalance_Folds\u001b[1;34m(BF, inData, classData, minClass, minSize)\u001b[0m\n\u001b[0;32m     16\u001b[0m Output_folds \u001b[39m=\u001b[39m []\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(BF):\n\u001b[1;32m---> 19\u001b[0m     usedLines                    \u001b[39m=\u001b[39m balance(inData, classData, minClass, minSize)\n\u001b[0;32m     20\u001b[0m     input_balance, label_balance \u001b[39m=\u001b[39m balance_data(inData, classData, usedLines)\n\u001b[0;32m     22\u001b[0m     Input_folds\u001b[39m.\u001b[39mappend(input_balance)\n",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m, in \u001b[0;36mbalance\u001b[1;34m(inData, classData, minClass, minSize)\u001b[0m\n\u001b[0;32m     12\u001b[0m usedLines \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(inData)      \u001b[39m#Array of false for length of data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(inData)):\n\u001b[1;32m---> 14\u001b[0m     \u001b[39mif\u001b[39;00m classData[i] \u001b[39m==\u001b[39m minClass:       \u001b[39m#Balance directly with dataframe\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         usedLines[i] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m            \u001b[39m#True lines are SNP\u001b[39;00m\n\u001b[0;32m     17\u001b[0m usedCount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1012\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m   1011\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m-> 1012\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m   1014\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m   1015\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1017\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\series.py:1121\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1120\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1121\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1123\u001b[0m \u001b[39mif\u001b[39;00m is_integer(loc):\n\u001b[0;32m   1124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[loc]\n",
      "File \u001b[1;32mc:\\Users\\Shamin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 10"
     ]
    }
   ],
   "source": [
    "file = \"AC_dataset.csv\"\n",
    "\n",
    "# file_train                = input(\"Enter file for training: \")\n",
    "# file_test                 = input(\"Enter file for testing: \")\n",
    "file_train                  = \"STraining_Set.csv\"\n",
    "file_test                   = \"STesting_Set.csv\"\n",
    "Training_Set, Testing_Set, seed = open_data(file_train, file_test)                                #Create training and testing sets\n",
    "IT_list, LT_list, IV_list, LV_list = CV(Training_Set, seed)                                       #Cross-validate training set\n",
    "\n",
    "for folds in range(len(IT_list)):                                                       \n",
    "    classData                   = LT_list[folds]                                            #Training labels\n",
    "    inData                      = IT_list[folds]                                            #Training features\n",
    "    ValData                     = IV_list[folds]                                            #Validation features\n",
    "    Vallabel                    = LV_list[folds]                                            #Validation labels\n",
    "\n",
    "    minClass, minSize, maxSize  = find_minority_class(classData)                            #Determines imbalance\n",
    "    BF                          = Balance_ratio(maxSize, minSize)                           #Determins number of balancing folds needed\n",
    "    Input_folds, Output_folds   = Balance_Folds(BF, inData, classData, minClass, minSize)   # balance() and balance_data() functions are called under this\n",
    "    \n",
    "    # space                   = hyperopt_space()\n",
    "    # trials                  = Trials()\n",
    "    # fmin_objective          = partial(objective,\n",
    "    #                                     d_train_list    = d_train_list[randrange(BF)],\n",
    "    #                                     d_val           = d_val,\n",
    "    #                                     MCC_eval_metric = MCC_eval_metric)\n",
    "    # best                    = fmin(fn = fmin_objective,\n",
    "    #                                 space             = space,\n",
    "    #                                 algo              = tpe.suggest,\n",
    "    #                                 max_evals         = 120,\n",
    "    #                                 trials            = trials,\n",
    "    #                                 )\n",
    "    \n",
    "    # final_param.append(trials.argmin)\n",
    "    \n",
    "    BF_RFC                      = BF_fitting(BF, Input_folds, Output_folds)\n",
    "    Prob_matrix                 = BF_validate(BF_RFC, ValData)\n",
    "\n",
    "    Final_vote, Sum_PD, Sum_SNP = Weighted_Vote(Prob_matrix)\n",
    "\n",
    "    evalutation(Vallabel, Final_vote)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e5cd67c8584618c148c6f2b57de13817422ccd98975b320089863a41752ead79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
