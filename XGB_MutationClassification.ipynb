{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fcba82d",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d94d2bb",
   "metadata": {},
   "source": [
    "Example 2 is inbalanced data set; ~2200 in PD and ~1100 in SNP\n",
    "    Goal is to predict if mutation is SNP or PD\n",
    "    XG Boost\n",
    "        \n",
    "    Total samples: 3368\n",
    "    2254 PD samples\n",
    "    1111 SNP samples\n",
    "    3 NA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5737f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Imports the required libraries and packages \"\"\"\n",
    "\n",
    "import pandas as pd                                                              # Data manipulation in dataframes\n",
    "import numpy as np                                                               # Array manipulation\n",
    "import xgboost as xgb                                                            # Gradient boosting package\n",
    "\n",
    "import random as rd                                                              # Random seed generation\n",
    "import time                                                                      # Time program run time\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    matthews_corrcoef,                                                           # MCC for evaluation\n",
    "    # balanced_accuracy_score, #hyperparameter evaluation\n",
    "    # f1_score,  #hyperparameter evaluation\n",
    "    confusion_matrix,                                                            # Confusion matrix for classification evalutation\n",
    "    classification_report                                                        # Return the F1, precision, and recall of a prediction\n",
    "    )\n",
    "\n",
    "from sklearn.model_selection import(\n",
    "    train_test_split,                                                            # Splits data frame into the training set and testing set\n",
    "    # GridSearchCV,  # Searches all hyperparameters\n",
    "    # RandomizedSearchCV, # Searches random range of hyperparameters\n",
    "    GroupKFold                                                                   # K-fold CV with as groups\n",
    "        )\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "# from sklearn.ensemble import RandomForestClassifier                              # SK learn API for classificastion random forests\n",
    "\n",
    "np.set_printoptions(precision = 3,threshold=np.inf, suppress=True)               # Full array printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb451c9e",
   "metadata": {},
   "source": [
    "### Split dataset into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "bbfacd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Test_Split(file):\n",
    "    \"\"\"      \n",
    "    Input:      file             Pre-processed dataset done by PDB2AC script\n",
    "\n",
    "    Returns:    Training_Set     80% training set split\n",
    "                Testing_Set      20% testing set split\n",
    "                \n",
    "    80% training and 20% testing split. Splits are shuffled randomly and index reset\n",
    "    \"\"\"\n",
    "    AC_dataset                  = pd.read_csv(file, index_col=0)  \n",
    "    Training_Set, Testing_Set   = train_test_split(AC_dataset,train_size = 0.8)\n",
    "        \n",
    "    Training_Set                = Training_Set.sample(frac = 1) #Shuffle data after splitting\n",
    "    Testing_Set                 = Testing_Set.sample(frac = 1)\n",
    "    \n",
    "    Training_Set.reset_index(drop=True, inplace = True) #Drop index to avoid training on index values\n",
    "    Testing_Set.reset_index(drop=True, inplace = True)  #Reset index after splitting for compatability with group fold CV\n",
    "    \n",
    "    return Training_Set, Testing_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "89126467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(Training_Set, Testing_Set):\n",
    "    \"\"\"      \n",
    "    Input:      Training_Set     80% training set split\n",
    "                Testing_Set      20% testing set split\n",
    "\n",
    "    Returns:    train_features   Features for training\n",
    "                train labels     Class lables for training\n",
    "                test_features    Features for testing\n",
    "                test_labels      Class labels for testing\n",
    "                \n",
    "    Creates the datasets needed for GBC model training and predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    train_features     = Training_Set.drop(['AC Code','dataset'], axis =1)      \n",
    "    train_labels       = Training_Set['dataset']                                  \n",
    "    groups             = Training_Set['AC Code'].to_list()                        #List of proteins for grouping\n",
    "    \n",
    "    test_features     = Testing_Set.drop(['AC Code','dataset'], axis =1)         \n",
    "    test_labels       = Testing_Set['dataset']                                  \n",
    "        \n",
    "    return(train_features, train_labels, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8b7f5",
   "metadata": {},
   "source": [
    "### Initial evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cf8d9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(inData, classData, ValData, Vallabel):\n",
    "    \"\"\" \n",
    "    Input:  Training_Set   Training data\n",
    "            Testing_Set    Testing data\n",
    "\n",
    "    Evaluate training data before CV and balancing. Gradient boosting for prediction on the test data. \n",
    "    True values are testing data class labels\n",
    "    \"\"\"    \n",
    "    d_train = xgb.DMatrix(inData, classData)\n",
    "    d_test = xgb.DMatrix(ValData, Vallabel)\n",
    "\n",
    "    params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:hinge', \n",
    "    }\n",
    "    XGB_initial = xgb.train(params, d_train)\n",
    "    \n",
    "    Output_pred = XGB_initial.predict(d_test)\n",
    "    print(f\"              **Initial Evaluation**\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Vallabel, Output_pred)}\")\n",
    "    print(f\"MCC              {matthews_corrcoef(Vallabel, Output_pred)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b3a2df",
   "metadata": {},
   "source": [
    "## Group K-fold CV (outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "975ff775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CV(Training_Set):\n",
    "#     \"\"\"      \n",
    "#     Input:      Training_Set     80% training set split\n",
    "            \n",
    "#     Returns:    IT_list         List of training features for each fold\n",
    "#                 LT_list         List of training class labels for each fold\n",
    "#                 IV_list         List of validation features for each fold\n",
    "#                 LV_list         List of validation class labels for each fold\n",
    "\n",
    "#     K-fold CV with protein groups separated between training and validation sets for each fold. Creates 5 folds.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     features     = Training_Set.drop(['AC Code','dataset'], axis =1)         #Features for training\n",
    "#     labels       = Training_Set['dataset']                         #Class labels for training\n",
    "#     groups       = Training_Set['AC Code'].to_list()               #List of proteins for grouping\n",
    "    \n",
    "#     params = {\n",
    "#     'booster': 'gbtree',\n",
    "#     'objective': 'binary:hinge', \n",
    "#     }\n",
    "    \n",
    "#     d_train = xgb.DMatrix(features, labels)\n",
    "#     xgb_model = xgb.train(params, d_train)\n",
    "        \n",
    "#     CV = xgb.cv(\n",
    "#         params=params,\n",
    "#         dtrain = d_train,\n",
    "#         nfold = 5,\n",
    "#         early_stopping_rounds= 20,\n",
    "#         metrics='error',\n",
    "#         as_pandas=True,\n",
    "#     )\n",
    "    \n",
    "#     print(CV)\n",
    "        \n",
    "#     # CV             = GroupKFold(n_splits = 5)                           #Creates 5 splits\n",
    "    \n",
    "#     # IT_list = []\n",
    "#     # LT_list = []\n",
    "#     # IV_list = []\n",
    "#     # LV_list = []\n",
    "    \n",
    "#     # for train_idx, val_idx in CV.split(Input_CV, Output_CV, Protein_Groups): #Generates the indices to be used for a training and validation split. Indicies are unique to train/ val sets\n",
    "        \n",
    "#     #     Rd = np.random.randint(time.time())                                  #Random number from 1 to time since epoch\n",
    "\n",
    "#     #     Input_train                        = Input_CV.loc[train_idx]         #New dataframe from selected indices\n",
    "#     #     Classes_train                      = Output_CV.loc[train_idx]\n",
    "#     #     Input_train.drop(['AC Code'], axis = 1, inplace = True)              #Group identifer not needed for training\n",
    "                \n",
    "#     #     Input_val                          = Input_CV.loc[val_idx]\n",
    "#     #     Classes_val                        = Output_CV.loc[val_idx]\n",
    "#     #     Input_val.drop(['AC Code'], axis   = 1, inplace = True)\n",
    "\n",
    "#     #     IT_list.append(Input_train.sample(frac=1, random_state=Rd))          #Shuffles lists, random state to ensure features and labels match for each fold\n",
    "#     #     LT_list.append(Classes_train.sample(frac=1, random_state=Rd))\n",
    "#     #     IV_list.append(Input_val.sample(frac=1, random_state=(Rd-1)))\n",
    "#     #     LV_list.append(Classes_val.sample(frac=1, random_state=(Rd-1)))\n",
    "        \n",
    "\n",
    "#     return(xgb_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335a585",
   "metadata": {},
   "source": [
    "## Balancing (inner loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6b6924e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_minority_class(classData):\n",
    "    \"\"\" \n",
    "    Input:        classData  Array of class labels\n",
    "\n",
    "    Returns:      minClass   The label for the minority class\n",
    "                  minSize    The number of items in the minority class\n",
    "                  maxSize    The number of items in the majority class\n",
    "\n",
    "    Find information about class size imbalance\n",
    "    \"\"\"\n",
    "    \n",
    "    Minority_count = 0\n",
    "    Majority_count = 0\n",
    "    for datum in classData:\n",
    "        if datum == 1:\n",
    "            Majority_count += 1\n",
    "        elif datum == 0:\n",
    "            Minority_count += 1\n",
    "\n",
    "    minClass = 0\n",
    "    minSize  = Minority_count\n",
    "    maxSize  = Majority_count\n",
    "    if Minority_count > Majority_count:\n",
    "        minClass = 1\n",
    "        minSize  = Majority_count\n",
    "        maxSize  = Minority_count\n",
    "\n",
    "    return minClass, minSize, maxSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5d1241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(inData, classData, minClass, minSize):\n",
    "    \"\"\" \n",
    "    Input:        inData          array of input data\n",
    "                  classData       array of classes assigned\n",
    "                  minorityClass   class label for the minority class\n",
    "                  minoritySize    size of the minority class\n",
    "\n",
    "    Returns:      usedLines       array of indexes that are of interest for a balanced dataset\n",
    "\n",
    "    Perform the actual balancing for a fold between SNPs and PDs\n",
    "    \"\"\"\n",
    "    usedLines = [False] * len(inData) #Array of false for length of data\n",
    "    for i in range(len(inData)):\n",
    "        if classData[i] == minClass:\n",
    "            usedLines[i] = True            #True lines are SNP\n",
    "            \n",
    "    usedCount = 0\n",
    "    while usedCount < minSize:\n",
    "        i = rd.randrange(len(inData))\n",
    "        if usedLines[i] == False:\n",
    "            usedLines[i] = True\n",
    "            usedCount += 1          #Set PD lines \"True\", until equal to number of SNP lines\n",
    "\n",
    "    return usedLines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "d5c54edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_data(inData, classData, usedLines):\n",
    "    \"\"\"     \n",
    "    Input:      inData      array of input training data\n",
    "                classData   array of classes assigned to training data\n",
    "                usedLines   array of line indexes to print\n",
    "\n",
    "    Returns:    input_balance  Dataframe of balanced training features\n",
    "                label_balance  Dataframe of balanced training labels\n",
    "                       \n",
    "    Create dataframe of the input training data and classes used. Index_list preserves the indicies between usedLines and inData, used to pull the needed lines.\n",
    "    \"\"\"\n",
    "    Rd = np.random.randint(time.time())\n",
    "    index_list = []\n",
    "    \n",
    "    for i in range(len(usedLines)):\n",
    "        if usedLines[i] == True:\n",
    "            index_list.append(i)\n",
    "             \n",
    "    input_balance = inData.iloc[index_list] \n",
    "    label_balance = classData.iloc[index_list]   \n",
    "    \n",
    "    input_balance = input_balance.sample(frac=1, random_state=Rd).reset_index(inplace = False, drop = True)\n",
    "    label_balance = label_balance.sample(frac=1, random_state=Rd).reset_index(inplace = False, drop = True)\n",
    "    \n",
    "    return input_balance, label_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6746be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Balance_ratio(maxSize, minSize): \n",
    "    \"\"\" \n",
    "    Input:      maxSize     The number of items in the majority class\n",
    "                minSize     The number of items in the minority class\n",
    "\n",
    "    Returns:    BF          Number of balancing folds\n",
    "\n",
    "    Calculate the number of balancing folds needed using ratio of majority to minority class size. Double to ensure sufficient\n",
    "    majority class instances are sampled, then + 1 to make odd to allow weighted vote.\n",
    "    \"\"\"\n",
    "    Divide = maxSize/minSize\n",
    "    BF = (2 * round(Divide)) + 1 #Double ratio to nearest integer\n",
    "    return BF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "12239dc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Balance_Folds(BF, inData, classData, minClass, minSize):\n",
    "    \"\"\" \n",
    "    Input:      BF                Number of balancing folds\n",
    "                inData            Features for training\n",
    "                classData         Class labels for training\n",
    "                minClass          The label for the minority class\n",
    "                minSize           The number of items in the minority class\n",
    "                                  \n",
    "    Returns:    Input_folds       List of balanced training feature folds\n",
    "                Output_folds      List of balanced training label folds\n",
    "\n",
    "    Perform the balance_data() function n number of balancing fold times. Return lists for feature data and labels\n",
    "    where each item is the output of balance_data()\n",
    "    \"\"\"\n",
    "    Input_folds  = []\n",
    "    Output_folds = []\n",
    "\n",
    "    for i in range(BF):\n",
    "        usedLines                    = balance(inData, classData, minClass, minSize)\n",
    "        input_balance, label_balance = balance_data(inData, classData, usedLines)\n",
    "        \n",
    "        Input_folds.append(input_balance)\n",
    "        Output_folds.append(label_balance)\n",
    "            \n",
    "    return Input_folds, Output_folds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21cd1aaa",
   "metadata": {},
   "source": [
    "### XGB hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0840d9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def Hyperparameter(BF, Input_folds, Output_folds):\n",
    "#     \"\"\" Input:      BF                Number of balancing folds needed\n",
    "#                     Input_folds       List of 5 balanced arrays of training data\n",
    "#                     Output_folds      List of 5 balanced arrays of training data's labels\n",
    "\n",
    "#         Returns:    BF_RFC_HP         List of optimized hyperparameters for each RFC\n",
    "\n",
    "#         Perform RandomSearchCV on each RFC to optimize number of trees, max depth and max samples\n",
    "#     \"\"\"  \n",
    "#     estimator = RandomForestClassifier()\n",
    "#     param_grid = {\n",
    "#                 'n_estimators':np.arange(50,500,50),\n",
    "#                 'max_depth': np.arange(2, 10, 2),\n",
    "#                 'max_samples': np.arange(0.2, 1.2, 0.2)\n",
    "#                   }\n",
    "#     BF_RFC_HP = []\n",
    "\n",
    "#     for i in range(BF):\n",
    "#         HPtuning = RandomizedSearchCV(\n",
    "#             estimator,\n",
    "#             param_grid, \n",
    "#             scoring = 'balanced_accuracy',\n",
    "#             cv = 10,\n",
    "#             n_jobs = 6, #how many cores to run in parallel\n",
    "#             verbose = 2\n",
    "#             ).fit(Input_folds[i], Output_folds[i])\n",
    "#         BF_RFC_HP.append(HPtuning.best_params_)\n",
    "    \n",
    "#     return(BF_RFC_HP)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "796af0e8",
   "metadata": {},
   "source": [
    "### Train XGB on the trainings folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1decd7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_fitting(BF, Input_folds, Output_folds, ValData, Vallabel): \n",
    "    \"\"\" \n",
    "    Input:      BF                Number of balancing folds                      \n",
    "                Input_folds       List of balanced training feature folds\n",
    "                Output_folds      List of balanced training label folds\n",
    "\n",
    "    Returns:    BF_GBC            List of gradient boosted trees trained on each balancing fold\n",
    "\n",
    "    Create GBC model that returns probability predictions for each fold, using output of Balance_Folds() as training data\n",
    "    \"\"\"    \n",
    "    d_test = xgb.DMatrix(ValData, Vallabel)\n",
    "    \n",
    "    params = {\n",
    "    'booster': 'gbtree',\n",
    "    'objective': 'binary:logistic', \n",
    "    'eval_metric': ['error', 'logloss'],\n",
    "    'verbosity': 2,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    BF_GBC = []\n",
    "    for i in range(BF):\n",
    "        d_train = xgb.DMatrix(Input_folds[i], Output_folds[i]) #Create DMatrix for each training balanced fold\n",
    "        BF_GBC.append(xgb.train(params, \n",
    "                                d_train, \n",
    "                                evals  = [(d_test,'eval'), (d_train,'train')],\n",
    "                                verbose_eval = True, \n",
    "                                early_stopping_rounds=10,\n",
    "                                )\n",
    "                      ) #Generates and fits a GBC for each training balanced fold\n",
    "        \n",
    "    return BF_GBC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0dd278c",
   "metadata": {},
   "source": [
    "#### Validate each GBC on validation set, for each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "acc41cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BF_validate(BF_GBC, ValData):\n",
    "    \"\"\" \n",
    "    Input:      BF_RFC          List of RFCs trained on balancing folds\n",
    "                ValData         Unseen validation features from CV fold\n",
    "                \n",
    "    Returns:    Prob_matrix     List of arrays. Each item is 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "    \n",
    "    Test the trained RFCs on the test set, then for every instance, outputs the predicted probability for each class\n",
    "    \"\"\"\n",
    "    \n",
    "    Prob_matrix = []\n",
    "    \n",
    "    for i in range(len(BF_GBC)):\n",
    "        dtest = xgb.DMatrix(ValData)\n",
    "        Prob = BF_GBC[i].predict(dtest) #Predicts the probability of an instance belonging to the major/ positive class (PD/ 1). Output has shape (n_predictions,)\n",
    "        Prob_matrix.append(Prob)   \n",
    "        \n",
    "    return Prob_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b8fdd",
   "metadata": {},
   "source": [
    "### Weighted voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "71033215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weighted_Vote(Prob_matrix):\n",
    "    \"\"\" \n",
    "    Input:      Prob_matrix     List of arrays. 2D matrix where the 1st dimension is each subset in balancing fold, \n",
    "                                2nd dimension is predicted probability\n",
    "\n",
    "    Returns:    Final_vote      Weighted vote classification\n",
    "\n",
    "    Calculate the final weighted vote using confidence scores (Sc) from Prob_matrix. Binary classification formula:\n",
    "    Sc = (S0 -T)/(1-T) if S0> T\n",
    "    Sc = (T-S0)/T if S0 < T\n",
    "    \"\"\"\n",
    "    PD_prob_matrix = Prob_matrix \n",
    "    \n",
    "    SNP_prob_matrix = []\n",
    "    for i in range(len(Prob_matrix)):\n",
    "        sub = 1 - Prob_matrix[i]\n",
    "        SNP_prob_matrix.append(sub)\n",
    "                                            #**Predictor states its prediction/ confidence scores are between 0.0 and 1.0 for each class**#\n",
    "    \n",
    "    Sum_SNP = np.sum(SNP_prob_matrix, axis = 0)     #Sum of all SNP confidence scores. 1D Array\n",
    "    Sum_PD  = np.sum(PD_prob_matrix, axis = 0)      #Sum of all PD confidence scores. 1D Array\n",
    "    \n",
    "                                                        #**Predictor outputs 0. . . 1 as a prediction**#\n",
    "                                                        \n",
    "    # T = 0.45               #Lower threshold gives more sensitivity to PDs over SNPs\n",
    "    # Sc_SNP = []\n",
    "    # Sc_PD = []\n",
    "\n",
    "    # for fold in range(len(Prob_matrix)):        #Calculates SNP Sc all instances in each fold\n",
    "\n",
    "    #     Sc_SNP_fold = []                        #List of the Sc for each fold\n",
    "    #     for value in range(len(SNP_prob_matrix[fold])):\n",
    "    #         S0 = Prob_matrix[fold][value]  #Each SNP's confidence in prob matrix fold\n",
    "    #         if S0 < T:\n",
    "    #             Sc = (T - S0)/T\n",
    "    #         elif S0 >= T:\n",
    "    #             Sc = (S0 - T)/(1 - T)        \n",
    "    #         Sc_SNP_fold.append(Sc)              #List of Sc for each fold\n",
    "    #     Sc_SNP.append(Sc_SNP_fold)              #List of folds with Sc\n",
    "\n",
    "    # for fold in range(len(PD_prob_matrix)):        #Calculates PD Sc all instances in each fold\n",
    "    #     Sc_PD_fold = []\n",
    "    #     for value in range(len(Prob_matrix[fold])):\n",
    "    #         S0 = Prob_matrix[fold][value]  #Each PD's confidence in prob matrix fold\n",
    "    #         if S0 < T:\n",
    "    #             Sc = (T - S0)/T\n",
    "    #         elif S0 >= T:\n",
    "    #             Sc = (S0 - T)/(1 - T)        \n",
    "    #         Sc_PD_fold.append(Sc)\n",
    "    #     Sc_PD.append(Sc_PD_fold)\n",
    "    \n",
    "    Vote_arr  = [] \n",
    "\n",
    "    for i in range(len(Sum_PD)):\n",
    "        if Sum_PD[i] >= Sum_SNP[i]:\n",
    "            Vote_arr.append([1])                #Append PD classifications to list\n",
    "        elif Sum_SNP[i] > Sum_PD[i]:\n",
    "            Vote_arr.append([0])                #Append SNP classifications to list\n",
    "\n",
    "    Final_vote = np.stack(Vote_arr)             #Converts list of arrays to a 2D array\n",
    "    Final_vote = Final_vote.ravel()             #Flattens 2D array to 1D array\n",
    "\n",
    "    return(Final_vote, Sum_PD, Sum_SNP)         #Returns the final confidence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2ae1157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Final_score(Sum_PD, Sum_SNP, BF):\n",
    "#     \"\"\" \n",
    "#     Input:      Sum_PD      Sum of confidence score for PD predictions\n",
    "#                 Sum_SNP     Sum of confidence score for SNP predictions\n",
    "\n",
    "#     Returns:    S_out       Final confidence score\n",
    "\n",
    "#     Calculate the final confidence score\n",
    "#     \"\"\"\n",
    "    \n",
    "#     S_Out = np.abs((Sum_PD - Sum_SNP) /(BF*2))\n",
    "        \n",
    "#     return S_Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "92f36545",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evalutation(Final_vote, Vallabel):\n",
    "    \"\"\" \n",
    "    Input:      Testing_Set        Unseen 20% testing data\n",
    "\n",
    "    Evaluate each fold with confusion matrix and MCC\n",
    "    \"\"\"\n",
    "\n",
    "    Output_pred = Final_vote\n",
    "    MCC = matthews_corrcoef(Vallabel, Output_pred)\n",
    "    print(f\"-----------------------------------------------------\\n              ***Fold Evaluation***\\n\")\n",
    "    print(f\"Confusion Matrix:\\n {confusion_matrix(Vallabel, Output_pred)}\")\n",
    "    print(f\"{classification_report(Vallabel, Output_pred)}\\nMCC                           {MCC}\\n\")\n",
    "    \n",
    "    return MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "53993642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(Score_list):\n",
    "     \"\"\" \n",
    "     Input:      Score_list        List of MCC scores\n",
    "\n",
    "     Plots the MCCs of n runs, and calculates the average MCC\n",
    "     \"\"\"\n",
    "     fig, ax = plt.subplots(figsize=(16,10), dpi= 65)\n",
    "     x_axis = range(len(Score_list))\n",
    "     y_axis = Score_list\n",
    "\n",
    "     plt.scatter(x_axis, y_axis, color = 'teal')\n",
    "     plt.axhline(y=np.nanmean(Score_list), color = 'red', linestyle = 'dotted', linewidth = '1', label ='Avg')\n",
    "     plt.title('MCC of 15 XG Boost runs, no CV')\n",
    "     plt.xlabel('Run Number')\n",
    "     plt.ylabel('MCC')\n",
    "     plt.legend()\n",
    "     plt.show()\n",
    "     print(f\"Average: {np.nanmean(Score_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa67e232",
   "metadata": {},
   "source": [
    "### Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "9a74965e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 102 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]\teval-error:0.33284\teval-logloss:0.63806\ttrain-error:0.19460\ttrain-logloss:0.58911\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-error:0.30758\teval-logloss:0.60548\ttrain-error:0.16929\ttrain-logloss:0.52338\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 104 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-error:0.30906\teval-logloss:0.58779\ttrain-error:0.14342\ttrain-logloss:0.47128\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]\teval-error:0.30461\teval-logloss:0.57029\ttrain-error:0.13723\ttrain-logloss:0.43485\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-error:0.29272\teval-logloss:0.55935\ttrain-error:0.12936\ttrain-logloss:0.40504\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-error:0.28975\teval-logloss:0.54880\ttrain-error:0.12430\ttrain-logloss:0.38223\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 72 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-error:0.27340\teval-logloss:0.53725\ttrain-error:0.11136\ttrain-logloss:0.35856\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 60 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-error:0.27192\teval-logloss:0.53207\ttrain-error:0.10067\ttrain-logloss:0.34339\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 34 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-error:0.25854\teval-logloss:0.51896\ttrain-error:0.09674\ttrain-logloss:0.33102\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 90 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-error:0.25260\teval-logloss:0.50806\ttrain-error:0.08718\ttrain-logloss:0.30635\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]\teval-error:0.36256\teval-logloss:0.64029\ttrain-error:0.20979\ttrain-logloss:0.59856\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-error:0.30906\teval-logloss:0.60541\ttrain-error:0.17773\ttrain-logloss:0.53186\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-error:0.32689\teval-logloss:0.58888\ttrain-error:0.16085\ttrain-logloss:0.48622\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]\teval-error:0.31798\teval-logloss:0.57647\ttrain-error:0.15354\ttrain-logloss:0.45124\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-error:0.30312\teval-logloss:0.55847\ttrain-error:0.13442\ttrain-logloss:0.41249\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-error:0.30461\teval-logloss:0.54947\ttrain-error:0.12992\ttrain-logloss:0.38997\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-error:0.28975\teval-logloss:0.53681\ttrain-error:0.11249\ttrain-logloss:0.36105\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 44 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-error:0.27786\teval-logloss:0.51814\ttrain-error:0.10292\ttrain-logloss:0.34347\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-error:0.27340\teval-logloss:0.51610\ttrain-error:0.09449\ttrain-logloss:0.32944\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-error:0.25557\teval-logloss:0.50089\ttrain-error:0.08774\ttrain-logloss:0.31328\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]\teval-error:0.32244\teval-logloss:0.63343\ttrain-error:0.20191\ttrain-logloss:0.59275\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-error:0.30758\teval-logloss:0.60242\ttrain-error:0.17829\ttrain-logloss:0.53535\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-error:0.29569\teval-logloss:0.57869\ttrain-error:0.16085\ttrain-logloss:0.48110\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 74 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]\teval-error:0.29866\teval-logloss:0.56554\ttrain-error:0.15129\ttrain-logloss:0.44914\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 92 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-error:0.28678\teval-logloss:0.54872\ttrain-error:0.12936\ttrain-logloss:0.40972\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-error:0.27637\teval-logloss:0.54279\ttrain-error:0.12205\ttrain-logloss:0.38736\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-error:0.28380\teval-logloss:0.53228\ttrain-error:0.11192\ttrain-logloss:0.36346\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 50 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-error:0.26597\teval-logloss:0.51272\ttrain-error:0.10124\ttrain-logloss:0.34215\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-error:0.25854\teval-logloss:0.50656\ttrain-error:0.09561\ttrain-logloss:0.33011\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 70 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-error:0.25557\teval-logloss:0.50350\ttrain-error:0.08774\ttrain-logloss:0.31386\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]\teval-error:0.31798\teval-logloss:0.63441\ttrain-error:0.19798\ttrain-logloss:0.59100\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-error:0.30461\teval-logloss:0.60760\ttrain-error:0.16985\ttrain-logloss:0.53132\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-error:0.30312\teval-logloss:0.58587\ttrain-error:0.14848\ttrain-logloss:0.47390\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 76 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]\teval-error:0.30461\teval-logloss:0.57891\ttrain-error:0.14229\ttrain-logloss:0.44098\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 86 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-error:0.28678\teval-logloss:0.56449\ttrain-error:0.12373\ttrain-logloss:0.40859\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 66 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-error:0.27340\teval-logloss:0.54782\ttrain-error:0.10855\ttrain-logloss:0.38101\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-error:0.26003\teval-logloss:0.54094\ttrain-error:0.10574\ttrain-logloss:0.35974\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 62 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-error:0.25557\teval-logloss:0.51539\ttrain-error:0.09674\ttrain-logloss:0.33308\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 84 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-error:0.24220\teval-logloss:0.50642\ttrain-error:0.08605\ttrain-logloss:0.30883\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 56 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-error:0.24220\teval-logloss:0.50594\ttrain-error:0.08324\ttrain-logloss:0.29687\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 88 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[0]\teval-error:0.32838\teval-logloss:0.63164\ttrain-error:0.20304\ttrain-logloss:0.59409\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 96 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[1]\teval-error:0.29718\teval-logloss:0.60011\ttrain-error:0.17435\ttrain-logloss:0.52870\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 100 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[2]\teval-error:0.30163\teval-logloss:0.57748\ttrain-error:0.15748\ttrain-logloss:0.47572\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[3]\teval-error:0.28678\teval-logloss:0.55813\ttrain-error:0.13723\ttrain-logloss:0.43139\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 78 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[4]\teval-error:0.29569\teval-logloss:0.55715\ttrain-error:0.12992\ttrain-logloss:0.40772\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 64 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[5]\teval-error:0.28380\teval-logloss:0.54228\ttrain-error:0.11642\ttrain-logloss:0.38229\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 94 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[6]\teval-error:0.26597\teval-logloss:0.53419\ttrain-error:0.10911\ttrain-logloss:0.35575\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 58 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[7]\teval-error:0.26449\teval-logloss:0.52855\ttrain-error:0.09955\ttrain-logloss:0.33545\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 82 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[8]\teval-error:0.25854\teval-logloss:0.52553\ttrain-error:0.08943\ttrain-logloss:0.31622\n",
      "[15:39:06] INFO: C:/buildkite-agent/builds/buildkite-windows-cpu-autoscaling-group-i-0fc7796c793e6356f-1/xgboost/xgboost-ci-windows/src/tree/updater_prune.cc:98: tree pruning end, 52 extra nodes, 0 pruned nodes, max_depth=6\n",
      "[9]\teval-error:0.26300\teval-logloss:0.52258\ttrain-error:0.08436\ttrain-logloss:0.30348\n",
      "-----------------------------------------------------\n",
      "              ***Fold Evaluation***\n",
      "\n",
      "Confusion Matrix:\n",
      " [[176  46]\n",
      " [106 345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.79      0.70       222\n",
      "           1       0.88      0.76      0.82       451\n",
      "\n",
      "    accuracy                           0.77       673\n",
      "   macro avg       0.75      0.78      0.76       673\n",
      "weighted avg       0.80      0.77      0.78       673\n",
      "\n",
      "MCC                           0.5314945644620442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = \"AC_dataset.csv\"\n",
    "# Score_list = []\n",
    "# for i in range(0,15):\n",
    "Training_Set, Testing_Set          = Train_Test_Split(file)                                 #Create training and testing sets\n",
    "inData, classData, ValData, Vallabel = data(Training_Set, Testing_Set)                     #training/ testing features and labels\n",
    "# xgb_model = CV(Training_Set)                                                              #Cross-validate training set   \n",
    "                           \n",
    "# test(inData, classData, ValData, Vallabel)                                                          #Initial evaluation\n",
    "\n",
    "minClass, minSize, maxSize  = find_minority_class(classData)                            #Determines imbalance\n",
    "BF                          = Balance_ratio(maxSize, minSize)                           #Determins number of balancing folds needed\n",
    "Input_folds, Output_folds   = Balance_Folds(BF, inData, classData, minClass, minSize)   # balance() and balance_data() functions are called under this\n",
    "# BF_RFC_HP = Hyperparameter(BF, Input_folds, Output_folds)\n",
    "BF_GBC                      = BF_fitting(BF, Input_folds, Output_folds, ValData, Vallabel)\n",
    "Prob_matrix                 = BF_validate(BF_GBC, ValData)\n",
    "\n",
    "Final_vote, Sum_PD, Sum_SNP = Weighted_Vote(Prob_matrix)\n",
    "# S_Out                       = Final_score(Sum_PD, Sum_SNP, BF)\n",
    "\n",
    "MCC = evalutation(Final_vote, Vallabel)\n",
    "\n",
    "# Score_list.append(MCC)    \n",
    "# plot(Score_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 337.844,
   "position": {
    "height": "359.844px",
    "left": "1536px",
    "right": "20px",
    "top": "112px",
    "width": "354px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e5cd67c8584618c148c6f2b57de13817422ccd98975b320089863a41752ead79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
